{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnjzW0-TPMvF"
   },
   "source": [
    "# Lab 2: Constrained minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import math as mt\n",
    "from numpy import linalg as ln\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction: <a id='I'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session, we present three different ways to numerically solve a convex, quadratic minimization problem with a single linear equality constraint:\n",
    "$$\n",
    "\\text{minimize } f(x)= \\frac{1}{2}x^TAx+b^Tx\\text{ over all }x\\in \\mathbb{R}^N\\text{ such that } c(x)=d^Tx-1=0,\n",
    "$$\n",
    "with $A$ symmetric positive definite, and $b,d\\in \\mathbb{R}^N$, with $d\\neq 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** Prove that there is a unique minimizer $x^*$. Write down the 1st order necessary condition for minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2** Deduce that $x^*=-A^{-1}(b+\\lambda d)$, with $\\displaystyle \\lambda=-\\frac{1+d^TA^{-1}b}{d^TA^{-1}d}$. \n",
    "\n",
    "Write programs with input $x$ that return $f(x)$, gf$=\\nabla f(x)$, $c(x)$ and gc$=\\nabla c(x)$, and one that returns $x^*$ and the Lagrange multiplier $\\lambda$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We will consider the case the case $N=2$ to allow figures, but codes should be written to allow arbitrari matrix sizes. For the tests and figures, use\n",
    "$$\n",
    "A=\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix},\\ b=\\begin{pmatrix}1\\\\3\\end{pmatrix},\\ d=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([[2,-1],[-1,2]])\n",
    "b=np.array([1,3])\n",
    "d=np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Solution 2: fill in the BLANKs\n",
    "def objective(x):\n",
    "    return np.sum(x*(0.5*A@x+b))\n",
    "\n",
    "def gradobj(x):\n",
    "    return BLANK\n",
    "\n",
    "def constraints(x):\n",
    "    return np.sum(d*x)-1\n",
    "\n",
    "def gradconstraints(x):\n",
    "    return BLANK\n",
    "\n",
    "def OptimSetup(x):\n",
    "    f=objective(x)\n",
    "    gf=gradobj(x)\n",
    "    c=constraints(x)\n",
    "    gc=gradconstraints(x)\n",
    "    return (f,gf,c,gc)\n",
    "\n",
    "def exactmin(A,b,d):\n",
    "    Amd= np.linalg.solve(A,d)\n",
    "    lamb=-(1.+np.sum(b*Amd))/np.sum(d*Amd)\n",
    "    solution=-np.linalg.solve(A,b+lamb*d)\n",
    "    y= objective(solution)\n",
    "    cons=constraints(solution)\n",
    "    return (solution,lamb,y,cons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** Compute $x^*, \\lambda$ and $y^*=f(x^*)$ in this case. Then, using code from the previous session, level sets of $f$ around $x^*$ (including that at value $y^*$), and the set defined by the constraints (1-level set of c) have all been drawn. \n",
    "\n",
    "Geometrically, what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol,lamb,y,cons=exactmin(A,b,d)\n",
    "print(solution,lamb,minf,cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "    z = f(*np.meshgrid(x, y))\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\n",
    "\n",
    "f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "c = lambda x, y: x+y-1\n",
    "\n",
    "level_lines(f, -5, 5, -5, 5, np.linspace(-4, 12, 17))\n",
    "level_lines(f, -5, 5, -5, 5, np.linspace(minf, minf, 1))\n",
    "level_lines(c, -5, 5, -5, 5, np.linspace(0, 0, 1))\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 3**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Penalization: <a id='I'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method consists in replacing the constrained minimization by an unconstrained, penalized minimization problem as follows:\n",
    "$$\n",
    "\\text{Minimize } f_\\rho(x)=f(x)+\\rho \\Vert c(x)\\Vert^2\\text{ over all }x\\in \\mathbb{R^N}.\n",
    "$$\n",
    "Here, we take $\\rho$ to be a large positive number. Notice that for linear constraints, $f$ convex implies $f_\\rho$ convex.\n",
    "\n",
    "In our case,\n",
    "$$\n",
    "f_\\rho(x)=\\frac{1}{2}x^TAx+b^Tx+\\rho(d^Tx-1)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** Show that \n",
    "$$\n",
    "\\nabla f_\\rho(x)=\\nabla f(x)+2\\rho c(x)\\nabla c(x)=(A+2\\rho dd^T)x+b-2\\rho d,\n",
    "$$\n",
    "and compute the minimizer $x^*_\\rho$ of $f_\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5** Code functions that compute, for any given $x$, the values of $f(x)$, $\\nabla f_\\rho(x)$ and $x_\\rho^*$. Verify numerically that $x_\\rho^*\\rightarrow x^*$ as $\\rho$ goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 5\n",
    "#Tip :  dd^T can be coded as        np.matmul(d[:,np.newaxis],d[np.newaxis,:])\n",
    "rho=10\n",
    "\n",
    "def objpen(x):\n",
    "    return objective(x)+rho*(constraints(x)**2)\n",
    "\n",
    "def gradobjpen(x):\n",
    "    return gradobj(x)+2*rho*constraints(x)*gradconstraints(x)\n",
    "\n",
    "def minpen(A,b,d,rho):\n",
    "    \n",
    "    xrho = \n",
    "    return xrho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##verify that as rho goes to infinity, xrho goes to the solution\n",
    "rho=\n",
    "xrho=minpen(A,b,d,rho)\n",
    "print(xrho,solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5** Write a code that uses gradient descent with a basic linesearch to minimize $f_\\rho$ in 1000 steps (forego the usual stopping criterion). \n",
    "\n",
    "Test it for $\\rho=1,10,100,1000$, drawing the trajectory of the sequence over the level sets of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 5: Fill in the BLANKS\n",
    "def optim(f, gradf, beta, gamma, x0, N): # beta and gamma are the linesearch parameters\n",
    "    xi = x0\n",
    "    X=[np.copy(x0)]\n",
    "    for i in range(N):\n",
    "        alpha = 1\n",
    "        d = BLANK\n",
    "        j=0\n",
    "        while f(xi + alpha * d) > f(xi) + gamma * alpha * np.sum(d* gradf(xi)) and j<20: #linesearch\n",
    "            j=j+1\n",
    "            alpha = beta * alpha\n",
    "        xi = BLANK\n",
    "        X.append(np.copy(xi))\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([0,0])\n",
    "beta=0.75\n",
    "gamma=0.5\n",
    "N=1000\n",
    "\n",
    "##plotting results for rho=1,10...\n",
    "for i in range(4):\n",
    "    rho=10**i\n",
    "    Xrho=optim(objpen, gradobjpen, beta, gamma, x0, N)\n",
    "    plt.figure()\n",
    "    plt.title(fr\"$\\rho=${rho}\")\n",
    "    f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "    c = lambda x, y: x+y-1\n",
    "    level_lines(f, -3, 3, -3, 3, np.linspace(-4, 12, 17))\n",
    "    level_lines(f, -3, 3, -3, 3, np.linspace(1.9166666666666667, 1.9166666666666667, 1))\n",
    "    level_lines(c, -3, 4, -3, 3, np.linspace(0, 0, 1))\n",
    "    plt.plot(Xrho[:,0],Xrho[:,1],'.',linestyle='-')\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6** Draw the level sets of $f_\\rho$ for $\\rho=1,10,100$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Solution 6\n",
    "for i in range(4):\n",
    "    plt.figure()\n",
    "    rho=10**i\n",
    "    f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y+rho*(x+y-1)**2\n",
    "    level_lines(f, -3, 3, -3, 3, np.linspace(-4, 12, 17))\n",
    "    plt.axis('equal')\n",
    "    plt.title(fr\"$\\rho=${rho}\")\n",
    "    plt.show()\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Projected Gradient: <a id='I'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (Bonus)** Let $C$ be a convex set subset of $\\mathbb{R}^N$. Prove that for any $x\\in\\mathbb{R}^N$, there is a unique point $P_C(x)\\in C$ such that for any $y\\in C$, $\\Vert x -P_C(x)\\Vert\\leq \\Vert x-y\\Vert$. In other words, $P_C(x)$ is the projection of $x$ onto $C$. \n",
    "\n",
    "When $C$ is an affine subspace, $P_C(x)$ is just the orthogonal projection onto $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Projected Gradient method consists in starting from a point $x_0$ in the constrained set $C$ and, at each step, applying the unconstrained gradient descent, followed by the projection of the descended point obtained back to the constrained set. So, for $k=0,1,2,\\ldots\\ $ up to convergence, repeat:\n",
    "\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{lcl}\n",
    "v^k& \\longleftarrow & -\\nabla f(x^k),\\\\\n",
    "\\alpha^k &\\longleftarrow &\\text{Line-search}\\ \\left(\\ t\\mapsto f(P_c(x^k + tv^k))\\ \\right),\\\\ \n",
    "x^{k+1}&\\longleftarrow &P_c(x^k+\\alpha^k v^k)\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "where $P_C$ is the projection operator onto the set of constraints $C=\\{x,\\ c(x)=0\\}$ defined in Question 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the linesearch step needs to be adjusted: instead of asking for \n",
    "$$\n",
    "f(x^k + tv^k)<f(x^k)-t\\gamma \\nabla f(x^k)^Tv^k,\n",
    "$$ \n",
    "we replace $v^k$ with $P(x^k+tv^k)-x^k$, so the condition becomes  \n",
    "$$\n",
    "f(P_C(x^k + tv^k))<f(x^k)-t\\gamma \\nabla f(x^k)^T(P_C(x^k+tv^k)-x^k)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: For a constraint like $C=\\{(0,x_2)\\in \\mathbb{R}^2\\}$, $P_C(x_1,x_2)=(0,x_2)$, so the algorithm becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left|\n",
    "\\begin{array}{lcl}\n",
    "v^k=\\begin{pmatrix}v^k_1\\\\v^k_2\\end{pmatrix}& \\longleftarrow & -\\nabla f(x^k),\\\\\n",
    "\\alpha^k &\\longleftarrow &\\text{Line-search}\\ \\left(\\ t\\mapsto f(P_c(x^k + tv^k))=\\ f\\begin{pmatrix}0\\\\x^k_2+tv^k_2\\end{pmatrix}\\right),\\\\ \n",
    "x^{k+1}&\\longleftarrow &\\begin{pmatrix}0\\\\x^k_2+\\alpha^kv^k_2\\end{pmatrix}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8** In our case, prove that $\\displaystyle P_C(x)=x-\\frac{d^Tx-1}{\\Vert d\\Vert^2}d$. \n",
    "\n",
    "Code the projection function $P_C(x)$ and test it on some points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 8** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 8 fill in the BLANK\n",
    "def proj(x):\n",
    "    return BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([0,0]) \n",
    "x1=np.array([1,2])\n",
    "x2=np.array([0.5,0.5])\n",
    "y0=proj(x0)\n",
    "y1=proj(x1)\n",
    "y2=proj(x2)\n",
    "print(constraints(y0),constraints(y1),constraints(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9** Write the projected gradient algorithm and test it for $N=10$ steps. Draw the sequence $(x_0,x_1,...)$ on a figure along with level sets of $f$ and the constraints set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution 9\n",
    "#tip: don't forget to ensure x0 belong to C (p is the projection function )\n",
    "#\n",
    "def optimproj(f, gradf, p, beta, gamma, x0, N, niter_ls_max): # beta and gamma are the linesearch parameters\n",
    "    X=[np.copy(x0)]\n",
    "    xi = p(x0)\n",
    "    X.append(np.copy(xi))\n",
    "    for i in range(N):\n",
    "        alpha = 1\n",
    "        d = BLANK\n",
    "        j=0\n",
    "        xtest = BLANK\n",
    "        while j<20 and f(xtest) > f(xi) + gamma * alpha * np.sum((BLANK)* gradf(xi)):\n",
    "            j=j+1\n",
    "            alpha = beta * alpha\n",
    "            xtest=BLANK\n",
    "        xi = xtest\n",
    "        X.append(np.copy(xi))\n",
    "    return np.array(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.array([0,0])\n",
    "beta=0.75\n",
    "gamma=0.5\n",
    "N=10\n",
    "niter_ls_max=20\n",
    "X=optimproj(objective, gradobj, proj, beta, gamma, x0, N, niter_ls_max)\n",
    "plt.figure()\n",
    "plt.title(fr\"Projected Gradient\")\n",
    "f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "c = lambda x, y: x+y-1\n",
    "level_lines(f, -3, 3, -3, 3, np.linspace(-4, 12, 17))\n",
    "level_lines(f, -3, 3, -3, 3, np.linspace(1.9166666666666667, 1.9166666666666667, 1))\n",
    "level_lines(c, -3, 4, -3, 3, np.linspace(0, 0, 1))\n",
    "plt.plot(X[:,0],X[:,1],'.',linestyle='-')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10** Can you think of circumstances where this method could be too computationnally intensive to be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Duality-based Lagrangian method: Uzawa update <a id='I'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is based on a result from class (to be seen next time). Define the Lagrangian\n",
    "$$\n",
    "\\mathcal{L}(x,\\lambda)=f(x)+\\lambda c(x),\\ x\\in \\mathbb{R}^N,\\ \\lambda\\in \\mathbb{R}.\n",
    "$$\n",
    "Note that for $x\\in C=\\{x,\\ c(x)=0\\}$, $\\mathcal{L}(x,\\lambda)=f(x)$ for every $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11** Let $(x^*,\\lambda^*)\\in \\mathbb{R}^N\\times\\mathbb{R}$ be the constrained minimizer and corresponding lagrange mutliplier:\n",
    "$$\n",
    "\\nabla f(x^*)+\\lambda^*\\nabla c(x^*)=0.\n",
    "$$\n",
    "Prove that $\\nabla\\mathcal{L}(x^*,\\lambda^*)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 11** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $(x^*,\\lambda^*)\\in \\mathbb{R}^N\\times\\mathbb{R}$ the constrained minimizer and corresponding lagrange mutliplier\n",
    "\n",
    "**Theorem:** For a convex functions and affine constraints,\n",
    "$$\n",
    "f(x^*)=\\max_{\\lambda\\in \\mathbb{R}}\\min_{x\\in \\mathbb{R}^N} \\mathcal{L}(x,\\lambda)=\\mathcal{L}(x^*,\\lambda^*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to an alternating method in which, at each step, we descend in $x$ then ascend in $\\lambda$: start with initial guesses $(x_0,\\lambda_0)$ for the Lagrange multiplier, then for $k=0,1,2,\\dots$, define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left|\n",
    "\\begin{array}{lcl}\n",
    "v^k& \\longleftarrow & -\\nabla_x \\mathcal{L}(x^k,\\lambda^k),\\\\\n",
    "\\mu^k& \\longleftarrow& \\Â \\nabla_\\lambda\\mathcal{L}(x^k,\\lambda^k),\\\\\n",
    "\\alpha^k &\\longleftarrow &\\text{Line-search}\\ \\left(\\ t\\mapsto \\mathcal{L}(x^k + tv^k,\\lambda^k)\\ \\right),\\\\ \n",
    "x^{k+1}&\\longleftarrow &x^k+\\alpha^k v^k,\\\\\n",
    "\\lambda^{k+1}&\\longleftarrow &\\lambda^k+s^0\\mu^k\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** $\\nabla_\\lambda\\mathcal{L}(x,\\lambda)=c(x).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12** For fixed $x$, what is the nature of $\\lambda\\mapsto \\mathcal{L}(x,\\lambda)$. Deduce that we can't perform a line-search for the step length of the update of the Lagrange multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13** Write a code that performs this algorithm for a given number $N$ of steps and $s_0$. Let it run on our problem for $N=100$ and $s=0.1$, $x_0=0$ and $\\lambda_0=0$. Draw the result on a figure that includes level sets of $f$ and the constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Solution 13\n",
    "def lagrangian(x,lam):\n",
    "    return BLANK\n",
    "\n",
    "def gradlagx(x,lam):\n",
    "    return BLANK\n",
    "\n",
    "def gradlaglam(x,lam):\n",
    "    return BLANK\n",
    "\n",
    "def optimlag(lag,gradx,gradlam,beta,gamma,x0,lam0,s0, N):\n",
    "    xi = x0\n",
    "    X=[x0]\n",
    "    lami=lam0\n",
    "    for i in range(N):\n",
    "        alpha = 1\n",
    "        d = BLANK\n",
    "        mu= BLANK\n",
    "        xtest=BLANK\n",
    "        j=0\n",
    "        while lag(xtest,lami) > lag(xi,lami) + gamma * alpha * np.sum((xtest-xi)* gradlagx(xi,lami)) and j<20:\n",
    "            alpha = beta * alpha\n",
    "            xtest=BLANK\n",
    "        xi = xtest\n",
    "        lami = BLANK\n",
    "        X.append(xi)\n",
    "    lamn=lami\n",
    "    return (np.array(X),lamn)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0.75\n",
    "alpha=0.5\n",
    "lam0=0\n",
    "x0=np.array([0,0])\n",
    "s0=0.1\n",
    "N=100\n",
    "X,lam = optimlag(lagrangian,gradlagx,gradlaglam,beta,gamma,x0,lam0,s0, N)\n",
    "plt.figure()\n",
    "plt.title(fr\"Uzawa Method\")\n",
    "f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "c = lambda x, y: x+y-1\n",
    "level_lines(f, -3, 3, -3, 3, np.linspace(-4, 12, 17))\n",
    "level_lines(f, -3, 3, -3, 3, np.linspace(1.9166666666666667, 1.9166666666666667, 1))\n",
    "level_lines(c, -3, 3, -3, 3, np.linspace(0, 0, 1))\n",
    "plt.plot(X[:,0],X[:,1],'.',linestyle='-')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[-1],lam)\n",
    "print(sol,lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 15** Try the algorithm with $s=1$ and $s_0=0.001$. Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam0=0\n",
    "x0=np.array([0,0])\n",
    "s0=1\n",
    "N=100\n",
    "X,lam = optimlag(lagrangian,gradlagx,gradlaglam,beta,gamma,x0,lam0,s0, N)\n",
    "plt.figure()\n",
    "plt.title(fr\"Uzawa Method\")\n",
    "f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "c = lambda x, y: x+y-1\n",
    "level_lines(f, -13, 3, -13, 3, np.linspace(-4, 12, 17))\n",
    "level_lines(f, -13, 3, -13, 3, np.linspace(1.9166666666666667, 1.9166666666666667, 1))\n",
    "level_lines(c, -13, 8, -13, 3, np.linspace(0, 0, 1))\n",
    "plt.plot(X[:,0],X[:,1],'.',linestyle='-')\n",
    "plt.axis('equal')\n",
    "\n",
    "s0=0.001\n",
    "N=100\n",
    "X,lam = optimlag(lagrangian,gradlagx,gradlaglam,beta,gamma,x0,lam0,s0, N)\n",
    "plt.figure()\n",
    "plt.title(fr\"Uzawa Method\")\n",
    "f = lambda x, y: 0.5*(2*x**2-2*x*y+2*y**2)+x+3*y\n",
    "c = lambda x, y: x+y-1\n",
    "level_lines(f, -13, 3, -13, 3, np.linspace(-4, 12, 17))\n",
    "level_lines(f, -13, 3, -13, 3, np.linspace(1.9166666666666667, 1.9166666666666667, 1))\n",
    "level_lines(c, -13, 8, -13, 3, np.linspace(0, 0, 1))\n",
    "plt.plot(X[:,0],X[:,1],'.',linestyle='-')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 15**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 16** Discuss each method's pros and cons, and which do you believe to be the best, both in general and for this specific minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "GafO0zXoJ6Cx",
    "5l_mvC1OJ6Da",
    "ZzS5-IzwaKn3",
    "89AjhkJ2aKoB"
   ],
   "name": "ToyNN_class.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
